{"cells":[{"cell_type":"markdown","source":["# Bronze Layer to Silver Layer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d116694-795c-4c5e-9d88-b2836073e25b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Importing modules and setting up oAuth for Azure"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9157eef3-f466-4e59-ae54-81d9bdb1aef4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import datetime\nimport re\nfrom pyspark.sql import types as T \nfrom pyspark.sql import functions as F \nfrom pyspark import StorageLevel"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"212bcdfd-557b-4a8d-9e08-18c8e368d8d9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["### Getting oAuth for Azure Container\nservice_credential = dbutils.secrets.get(scope=\"databricks\",key=\"databricks-test\")\nspark.conf.set(\"fs.azure.account.auth.type.ade20220919.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.ade20220919.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.ade20220919.dfs.core.windows.net\", \"98a69314-1a2f-4eef-9b09-6ff1e73ead68\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.ade20220919.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.ade20220919.dfs.core.windows.net\", \"https://login.microsoftonline.com/33da9f3f-4c1a-4640-8ce1-3f63024aea1d/oauth2/token\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14f6657d-6c5c-4e84-bfe3-77038f784173","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##  Defining Event Table types"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"121487fd-f7b7-4006-90b6-366779f603e5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["### List of Event Table types, we remove PublicEvent because it contains no columns\ntype_list_camel = ['PushEvent', \n             'GollumEvent', \n             'ReleaseEvent', \n             'CommitCommentEvent', \n             'CreateEvent', \n             'PullRequestReviewCommentEvent', \n             'IssueCommentEvent', \n             'DeleteEvent', \n             'IssuesEvent', \n             'ForkEvent',\n             'MemberEvent', \n             'WatchEvent', \n             'PullRequestEvent']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30d8abc1-94af-4e09-a582-82945cbf8fc0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Helper Functions for flattening & cleaning bronze layer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2918f86f-2cf3-465f-8e55-fa2a92c03b9c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["### Helper functions for flattening out data into a Fact Table and 14 Seperate Event Tables \n### with any array columns into their own separate tables\n\n## Flattening JSON Struct Functions --------------------------------------------------------\ndef event_df_flatten(big_df, event_type, limit_rows=True, L=500):    \n    '''\n    Extracts 'id', 'created_at', and 'payload' for one Event Table\n    from the Fact Table with 'payload' being nested\n\n    Flattens the event dataframe once, which flattens the payload column by one layer\n    then any null columns are dropped\n    after it contains the only payload columns that have data in it\n    then the dataframe is completely flattened \n\n    Limits rows by L if creating a template to save storage,\n    if template behavior not wanted, it does not limit the rows read\n\n    Params:\n        big_df : Dataframe\n            dataframe containing unnested columns with json structs in the 'payload' column\n        event_type : str\n            type of event that needs to be extracted and flattened into its own table\n        limit_rows : bool\n            primarlily used for making templates, \n            if True limits the amount of rows that are read in from the big_df\n            if False it reads in all the rows from big_df \n        L : int\n            number of rows that are read in from big_df if limit_rows is True\n\n    Returns:\n        Dataframe\n            Completely null column removed and flattened Event Table\n    '''\n    \n    # Filter out specific event type payload and flatten out one nested layer\n    if limit_rows:\n        event_df = big_df.filter(big_df.type == event_type).select('id','created_at','payload').limit(L)\n    else:\n        event_df = big_df.filter(big_df.type == event_type).select('id','created_at','payload')\n    \n    # Flatten one layer of payload, don't keep 'payload' in the column name\n    event_df = flatten_df(event_df, False)\n    \n    # Find and drop any columns that are completely full of null values\n    null_counts = event_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in event_df.columns]).collect()[0].asDict()\n    na_cols = [k for k in null_counts.keys() if null_counts[k] == L]   \n    event_df = event_df.drop(*na_cols)\n    \n    # Completely flatten all layers of the dataframe (other than the arrays)\n    return complete_flatten(event_df)\n\n\ndef flatten_df(nested_df, keep_parent_name = True):  \n    '''\n    Flattens one nested layer for every nested column in a DataFrame.\n\n    Ex: actor contains a schema for id, login, and url nested inside -> Flattens to actor_id, actor_login, actor_url\n\n    Params:\n        nested_df : DataFrame\n            DataFrame with nested information within columns\n    Returns: \n        flat_df : DataFrame\n            DataFrame with flattened columns\n    '''\n    \n    # store the column names that contain structs as their data\n    # in dtypes, 0 index is the col name, 1 index is the actual data\n    flat_cols = [F.col(x[0]) for x in nested_df.dtypes if not re.match('^struct',x[1])] \n    nested_cols = [x[0] for x in nested_df.dtypes if re.match('^struct',x[1])]\n    \n    if keep_parent_name:\n        # flatten nested columns using select(.*)\n        # names flattened columns as \"parentCol_childCol\"\n        flat_df = nested_df.select(flat_cols + [F.col(n+'.'+c).alias(n+'_'+c) \n                                                for n in nested_cols \n                                                for c in nested_df.select(n+'.*').columns])\n    else:\n        # names flattened columns as \"childCol\"\n        flat_df = nested_df.select(flat_cols + [F.col(n+'.'+c).alias(c) \n                                                for n in nested_cols \n                                                for c in nested_df.select(n+'.*').columns])\n        \n    return flat_df\n\n\ndef complete_flatten(nested_df):\n    '''\n    Flattens all nested layers in a DataFrame\n\n    Params:\n        nested_df : DataFrame\n            DataFrame with nested information within columns\n    Returns: \n        flat_df : DataFrame\n            DataFrame with completely flattened columns\n    '''\n    \n    flat_df = nested_df\n    # flatten until no columns are struct datatype\n    while any([re.match('^struct',x[1]) for x in flat_df.dtypes]):\n        flat_df = flatten_df(flat_df)\n    \n    return flat_df\n\n\n## Template Function  ------------------------------------------------------------------------------\ndef templatize(new_df, template_df_name, path):\n    '''\n    Reformats an Event or Array Table to contain only the columns from a Template DataFrame \n\n    Params:\n        new_df : DataFrame\n            Event or Array table to be reformatted by dropping columns from a Template DataFrame\n        model_df_name : str\n            name of Template DataFrame file\n        path : str\n            folder path to silver_layer\n    Returns:\n        new_df : DataFrame\n            Event or Array Dataframe matching the table structure of the Template DataFrame\n    '''\n    \n    # Read in Template dataframe\n    template_df = spark.read.parquet(path + model_df_name)\n    \n    # add null columns for columns that are in the template but not in the new_df\n    # this doesn't get used for project 2 but is for future-proofing the code\n    select_cols = new_df.columns + [F.lit(None).alias(c) for c in template_df.columns if c not in new_df.columns]\n    new_df = new_df.select(*select_cols)\n    \n    # Select all the corresponding columns in the input Dataframe that are in the Template dataFrame\n    new_df = new_df.select(template_df.columns)\n    \n    return new_df\n\n\n## Writing Seperate Event and Array Dataframe Files Functions ----------------------------------------------\ndef save_array_table(event_df, event_df_name, template, L, partition_by, path):\n    '''\n    Extracts Dimension Array Tables from an Event Table\n    flattens all structs in the Array Tables \n    writes Array Tables to a parquet file\n\n    Params:\n        event_df : DataFrame\n            Event Table dataframe\n        event_df_name : str\n            name of Event Table with underscores between each word\n        template : bool\n            if True saves L rows after dropping any unhelpful columns using drop_useless()\n            if False saves Event Table to a parquet file \n        L : int\n            number of rows to save to a Template Event Table if template is True\n        partition_by : str\n            if template is false then the Event Table parquet file is partitioned by this column\n        path : str\n            save path for the file\n    Returns:\n        DataFrame\n            Event Table with the array columns dropped\n    '''\n    \n    # Find any columns that are arrays\n    array_cols = [x[0] for x in event_df.dtypes if re.match('^array',x[1])]\n    \n    # If there are array columns, flatten them\n    if len(array_cols) > 0:\n        for a_col in array_cols:\n            array_df_name = event_df_name + '_' + a_col + '_df'\n            # Keep id as fact_id to tie back to Fact Table\n            # Keep created_at as event_created_at to be able to partition arrays by day\n            # explode array and flatten it\n            array_df = flatten_df(\n                event_df.select(F.col('id').alias('fact_id'),\n                F.col('created_at').alias('event_created_at'),\n                F.explode(a_col)), False)\n            # Make sure there are no structs left\n            array_df = complete_flatten(array_df)\n\n            if template:\n                # Save only L rows, drop any columns defined useless from drop_useless()\n                array_df = array_df.limit(L)\n                event_df = drop_useless(event_df)\n                array_df.write.mode('overwrite') \\\n                .parquet(path + array_df_name)\n            else:\n                # Choose only the columns deemed important using the Template Dataframe\n                array_df = templatize(array_df, array_df_name, path)\n                array_df = add_day(array_df, 'event_created_at')\n                array_df.write.partitionBy(partition_by).mode('overwrite') \\\n                .parquet(path + array_df_name)\n\n    # Drop array columns and return so Event Table doesn't contain repeat information from Array Tables     \n    return event_df.drop(*array_cols)\n\n\ndef save_event_table(event_df, event_df_name, template, L, partition_by, path):\n    '''\n    Writes Event Table to a parquet file\n\n    Params:\n        event_df : DataFrame\n            Event Table dataframe\n        event_df_name : str\n            name of Event Table with underscores between each word\n        template : bool\n            if True saves L rows after dropping any unhelpful columns using drop_useless()\n            if False saves Event Table to a parquet file \n        L : int\n            number of rows to save to a Template Event Table if template is True\n        partition_by : str\n            if template is false then the Event Table parquet file is partitioned by this column\n        path : str\n            save path for the file\n\n    ''' \n    \n    if template:\n        # Save only L rows, drop any columns defined useless from drop_useless()\n        event_df = event_df.limit(L)\n        event_df = drop_useless(event_df)\n        event_df.write.mode('overwrite').parquet(path + event_df_name)\n    else:\n        event_df.write.partitionBy(partition_by).mode('overwrite') \\\n        .parquet(path + event_df_name)\n     \n\n## Cleaning/Partitioning Helper Functions ------------------------------------------------------\ndef drop_useless(df, keep_list=[]):\n    '''\n    Returns a dataframe without columns contain \"gravatar\" or that end in \"url\"\n    url or gravatar columns can be kept if specified\n\n    Params:\n        df : DataFrame\n            dataframe to transform\n        keep_list : list of str\n            list of columns that contain \"gravatar\" or end in \"url\" that should be kept\n    Returns:\n        DataFrame with \"gravatar\" and \"url\" columns removed\n    '''\n    df = df.drop(*[col for col in df.columns if \n                    (col[-3:] == \"url\" or \"gravatar\" in col) and \n                    (col not in keep_list)])\n    return df\n\ndef add_day(df, date_col):\n    '''\n    Adds a day column to a DataFrame given a date column\n\n    Params:\n        df : DataFrame\n            dataframe to transform\n        date_col : str\n            string containing the column name with the date in it\n    Returns:\n        DataFrame\n            dataframe with day column\n    '''\n    return df.withColumn(\"day\", F.col(date_col).substr(1,10))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74f67289-6c8a-4789-abb7-da522e533686","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Creating Templates"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aff46444-f8d2-4dac-bdd1-15b7c8c5804f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["### Creating Template DataFrame Files\ntemplate_path = 'abfs://silver-layer@ade20220919.dfs.core.windows.net/miniro/templates/'\ntemplates_exist = len(dbutils.fs.ls(template_path)) != 0\n\nif not templates_exist:\n    # Read in just one day to make a template\n    template_file = f\"abfs://bronze-layer@ade20220919.dfs.core.windows.net/miniro/2017-01/day=2017-01-01\"\n    row_limit = 500\n    big_df = spark.read.parquet(template_file)\n\n    # Make templates for each event type table\n    for event_type in type_list_camel:\n        event_df_name = re.sub(r'(?<!^)(?=[A-Z])', '_', event_type).lower() + \"_df\"\n\n        # flatten, clean and save Event Table\n        event_df = event_df_flatten(big_df, event_type, True, row_limit)\n        # !! event tables are saved first on purpose here !!\n        # templates need to include array cols in the Event Table so that they can be written later\n        save_event_table(event_df, event_df_name, template=True, L=2, partition_by=None path=template_path)\n        \n        # flatten, clean and save Array Tables\n        event_df = save_array_table(event_df, event_df_name, template=True, L=2, partition_by=None, path=template_path) \n    \n    # Creating Fact Table as 'main_df'\n    main_df = flatten_df(big_df.drop(\"payload\"))\n    main_df = main_df.limit(2)\n    main_df = drop_useless(main_df)\n    main_df.write.mode('overwrite').parquet('abfs://silver-layer@ade20220919.dfs.core.windows.net/miniro/templates/main_df')   "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c1a047b-d8a5-4600-90e9-f5aede4b5910","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Splitting Data into flat and cleaned Fact Table, 14 Event Tables and Dimension Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a5e2516-0e0b-4ae7-bc4b-e8cfcf8d57c2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["### Creating Fact, Event, and Dimension Tables\nsilver_path = 'abfs://silver-layer@ade20220919.dfs.core.windows.net/miniro/'\n\n# Read in one month (can be changed to read in all the data)\nbig_df = spark.read.parquet(f\"abfs://bronze-layer@ade20220919.dfs.core.windows.net/miniro/2017-01/\")\nbig_df.cache()\n\n# Writing Event Table and Dimension array Tables files\nfor event_type in type_list_camel:\n    event_df_name = re.sub(r'(?<!^)(?=[A-Z])', '_', event_type).lower() + \"_df\"\n    event_df = event_df_flatten(big_df, event_type, limit_rows = False)\n    \n    # Select only the columns that are in the template df\n    event_df = templatize(event_df, event_df_name, template_path)\n    event_df = add_day(event_df, 'created_at')\n    event_df.cache()\n\n    # Drop arrays and save them as seperate tables\n    event_df = save_array_table(event_df, event_df_name, template=False, partition_by='day', 0, path=silver_path)\n\n    # Save event table to parquet after removing unnecessary columns\n    save_event_table(event_df, event_df_name, template=False, partition_by='day', 0, path=silver_path)\n    event_df.unpersist()\n\n# Flatten and clean Fact Table\nmain_df = flatten_df(big_df.drop(\"payload\"))\nmain_df = templatize(main_df, \"main_df\", template_path)\nmain_df = add_day(main_df, 'created_at')\n\n# Save Fact Table\nmain_df.write.partitionBy('day').mode('overwrite') \\\n.parquet(f'abfs://silver-layer@ade20220919.dfs.core.windows.net/miniro/main_df')\nbig_df.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2987e19b-3611-41d1-8d4b-0574025f0bda","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2f169c6-20c3-48bf-ac17-9a52a9c3e028","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"miniro_to_silver","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3453605860630690}},"nbformat":4,"nbformat_minor":0}
